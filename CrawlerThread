import threading
import queue
import requests
from bs4 import BeautifulSoup

class CrawlerThread(threading.Thread):
    def __init__(self, url_queue, visited, lock):
        super().__init__()
        self.url_queue = url_queue
        self.visited = visited
        self.lock = lock

    def run(self):
        while True:
            try:
                url = self.url_queue.get(timeout=5)
            except queue.Empty:
                break
            with self.lock:
                if url in self.visited:
                    self.url_queue.task_done()
                    continue
                self.visited.add(url)
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    print(f"Fetched: {url}")
                    soup = BeautifulSoup(response.text, "html.parser")
                    for link in soup.find_all("a"):
                        href = link.get("href")
                        if href and href.startswith("http"):
                            self.url_queue.put(href)
            except Exception as e:
                print(f"Error fetching {url}: {e}")
            self.url_queue.task_done()

if __name__ == "__main__":
    start_url = "https://www.example.com"
    url_queue = queue.Queue()
    url_queue.put(start_url)
    visited = set()
    lock = threading.Lock()
    threads = []
    for i in range(5):
        t = CrawlerThread(url_queue, visited, lock)
        t.start()
        threads.append(t)
    url_queue.join()
    for t in threads:
        t.join()
